---
title: "Projection analysis to measure biological elaboration and innovation."
author: "Thomas Guillerme, Natalie Cooper, Andrew P Beckerman,  Gavin H Thomas"
bibliography: references.bib
date: "`r Sys.Date()`"
output:
  html_document:
    fig_width: 8
    fig_height: 8
    keep_tex: true
  self_contained: true
---

```{r, echo = FALSE}
## For a fancy vignette
library(knitr)
```

This vignette explains the details of how the mini chains MCMCglmm method works and how to use its implementation in the [`mcmcmcglmmm`](https://github.com/TGuillerme/mcmcmcglmmm) package.
Note that this method is entirely based on Jarrod Hadfield's [`MCMCglmm` package](https://cran.r-project.org/web/packages/MCMCglmm/index.html) which fits generalised linear mixed models (GLMM) using Markov chain Monte Carlo techniques [@hadfield2010]. 
The `MCMCglmm` method **will not be explained in detail here**.
If you need to know more about this method, please refer to the excellent vignettes provided in the `MCMCglmm` package (a brief [overview](https://cran.r-project.org/web/packages/MCMCglmm/vignettes/Overview.pdf) and a more advanced set of [course notes](https://cran.r-project.org/web/packages/MCMCglmm/vignettes/CourseNotes.pdf)).

<!-- > If you use this vignette in published work, please cite @PAPER and @mcmcmcglmmmDOI. -->

#### Important note on the structure of this vignette

This vignette is intended to provide general details on `mcmcmcglmmm` that will be written in plain text (like this sentence).

```{r}
## This vignette also contains reproducible examples written in R snippets
this_is_a_reproducible_R_snippet <- TRUE
```

> And finally, details about specific parameters used in the paper for repeatability will be displayed in notes like this one.


## Installation and requirements

To install the `mcmcmcglmmm` package, you can directly download the latest version on GitHub using `devtools`:

```{r header, echo = TRUE, results = 'hide', message = FALSE, warning = FALSE, eval = FALSE}
## Installing the dispRity package from the CRAN
install.packages("dispRity")
## Installing the mcmcmcglmmm package from github
if(!require(devtools)) install.packages("devtools")
devtools::install_github("TGuillerme/mcmcmcglmmm")
```

```{r, echo = TRUE, results = 'hide', message = FALSE, warning = FALSE, eval = TRUE}
## Loading the packages
library(mcmcmcglmmm)
``` 

It also relies on the latest version of the [`dispRity`](https://github.com/TGuillerme/dispRity)) package (>=1.6.8) for some specific background functions.
The package will automatically install the latest versions of `MCMCglmm` and `dispRity`.

# Mini chains MCMCglmm method: an efficient way to calculate variance-covariance matrices on big data with phylogenetic uncertainty

One commonly used method to estimate variance-covariance matrices from phylogenetic datasets is to use generalised linear mixed models (GLMM).
This is often done in a Bayesian framework using the `MCMCglmm` package (@hadfield2010; cited nearly 4000 times in the last decade).
The MCMCglmm method allows you to run a GLMM on a multidimensional dataset with an underlying phylogenetic structure.
<!-- TG:TODO: Expand that from the longevity paper -->.

Using this method we can run a nested phylogenetic model on a multidimensional dataset (here the Charadriiformes PCA shapespace).
This model will have the multidimensional trait values as the response and the classic residual terms as an error term.
Additionally though, we can add the phylogenetic signal as a random error term (random effect).
This random error term can be expanded into several independent nested terms: one for the entire phylogenetic structure and one for each individual clade's phylogenetic structure.

$$\text{data} = \text{traits} + \text{residuals} + (\text{phylogenetic term} + \text{clade term})$$

Or in `MCMCglmm` pseudo code:

```
formula   = PC1:8 ~ trait:clade-1
random    = ~ us(at.level(clade):trait):animal + us(trait):animal
residuals = ~ us(trait):units
```
<!-- TG:TODO: explain that much better) -->

> In THE_PAPER we used a dataset of 8748 species and eight PC dimensions, and a model with 36 nested random terms: one for the whole phylogeny, eight for each super-order containing at least 15 species and 27 for each order containing at least 15 species and one residual term. % NC: This adds to 37...

Although this method is efficiently implemented on small to relatively large datasets, the major hurdle is the calculation of the variance-covariance matrix; this takes a length of time that increases following a power law with both the number of species and the number of dimensions.
Therefore, this method becomes easily untractable on large datasets (>8k species and >3 dimensions).
Furthermore, this method's phylogenetic correction is based on a single input tree (usually the consensus tree).
This can also be problematic for big trees (i.e. > 5k species) where the notion of a consensus tree being a good representation of phylogenetic hypotheses is at best misleading and at worst incorrect.
In fact, the bigger the tree, the more variance there is in the likely relationships among species (especially in terms of branch length towards deep branches).
Therefore, there has been a push to use tree distributions rather than consensus trees (i.e. point estimates) in phylogenetic GLMMs (e.g. @healy2014; @mulTree).
This however, leads to yet another computational time issue: the large dataset needs to be run on multiple trees (e.g. if using @mulTree) thus linearly increasing computational time.

Thus, to increase the speed of these analyses, while taking phylogenetic uncertainty into account, we used a parallelisable "mini chains" approach.
In brief, it runs multiple short `MCMCglmm` analyses on multiple trees and pulls the results together into one larger `MCMCglmm` that contains more variation due to phylogenetic uncertainty (where the size of the chains are optimised for speed and low RAM usage).

## Mini chains

The mini chains `MCMCglmm` method (`mcmcmcglmmm`) effectively works by running many very short parallel `MCMCglmm` on the same number of different tree topologies.
Effectively each mini chain generates a very small number of samples post-burnin (e.g. 10) and combines them across many replicates (e.g. 1000) into a chain containing effectively many samples (e.g. 10000), each run independently on a different tree topology.

![Figure 2](mini-chains_diagram.png)

##### Figure 2: {#fig2}
Mini chains diagram: the numerical values displayed here (number of chains, priors, etc...) are used as examples and should be tuned to fit specific questions and datasets.

In more detail, the method works as follows:

 1. Running a small number of "parameterisation chains": these chains are run with the trait data and the model of interest on a consensus tree from the tree distribution, along with flat priors with a low belief parameter (effectively not putting any important weight on the priors). 
 2. Extracting parameters from the "parameterisation chains": this step allows to extract the parameters of interest from the posteriors of the "parameterisation chains". The parameters of interest are the conservative burnin average time (here defined as highest number of iterations required to reach the median posterior likelihood across the parameterisation chains with an additional 10% extra generations) and the mini chains priors (the median posterior model results - ignoring the previously estimated burnin - with a slightly higher belief parameter than for the previous chains).
 3. Running the mini chains: these chains use the same data and model as the "parameterisation chains" but use a random tree from the tree distribution rather than the consensus one. Furthermore, they use the parameters previously estimated (i.e. the priors and the burnin phase). These chains are run for a small number of iterations past the burnin phase to extract a fixed number of samples; specifically the chain runs for $\text{burnin iterations} + \text{requested_samples} \times \text{sampling rate}$.
 4. Combining the mini chains: finally we can combine the post-burnin samples from all the mini chains back into a single classical `MCMCglmm` chain (with no burnin).

Each step is described and demonstrated in more detail below.

### Effectiveness
%NC: Wouldn't it be more sensible here to include info from this paper not suddenly introduce this other random paper?
In a practical implementation of the mini-chains for another project, we ran a similar model on 8748 species with eight dimensions and 31 nested levels.
Using this method, it took 20 parallel cores with 8GB of RAM, two months to complete the model with each individual mini-chain running for around 40 hours (using the SHARC cluster at University of Sheffield: CITE).
We estimated that the same model using the normal `MCMCglmm` implementation would have required 2.1 years and 4TB of RAM. 

This method thus allows us to run multivariate GLMMs on large datasets and across a tree distribution.
The posteriors of these analyses contain a number of variance-covariance matrices (`model$VCV` in a `MCMCglmm` output) and their location in the trait space (`model$Sol` in a `MCMCglmm` output).
We can then use these distributions of variance-covariance matrices as major axes in the multidimensional space on which to run the elaboration and exploration analyses.

# Detailed example

Here we illustrate the `mcmcmcglmmm` pipeline by applying it to a data set from @cooney2017 focused on the Charadriiformes order of birds.

## Data

Charadriiformes contains 359 species divided into three clades: gulls, sandpipers and plovers and sandpipers that have respectively 159, 102 and 98 species each.
For each of these species, we used the 3D beak dataset from @cooney2017 that is an ordination of the shapes of the beaks of 8748 birds (the resulting shape space used here does not contain any information about the beak size - i.e. centroid size).
We extracted the 359 charadriiform species from this space where the first three dimensions account for more than 99% of the variance in the dataset.
For the rest of the example analysis here, we used as a trait-space of these 359 species with three dimensions (hereafter the shapespace).

```{r, echo = FALSE, eval = TRUE}
## Loading the Charadriiformes data
data(charadriiformes)
## Extracting the tree
tree <- charadriiformes$tree
## Extracting the data column that contains the clade assignments
data <- charadriiformes$data[, "clade"]
## Changing the levels names (the clade names) to colours
levels(data) <- c("orange", "blue", "darkgreen")
data <- as.character(data)
## Matching the data rownames to the tip order in the tree
data <- data[match(ladderize(tree)$tip.label, rownames(charadriiformes$data))]

## Matching the tip colours (labels) to their descending edges in the tree
## (and making the non-match edges grey)
clade_edges <- match.tip.edge(data, tree, replace.na = "grey")

## Plotting the results
plot(ladderize(tree), show.tip.label = FALSE, edge.color = clade_edges, main = "Charadriiformes")
legend("bottomleft", lty = c(1,1,1), col = c("blue", "darkgreen", "orange"), legend = c("plovers", "sandpipers", "gulls"))
axisPhylo()
```

##### Figure 5: {#fig5}
The Charadriiformes data used in this pipeline example. The axis units are millions of years ago.


## Mini-chains MCMCglmm method

To illustrate the `mcmcmcglmmm` pipeline, we analysed the variance-covariance of the shapespace using a generalised linear mixed model with two different levels of random terms: one for the whole Charadriiformes phylogeny and one for each clade (gulls, plovers and sandpipers).
We first ran three parameterisation chains for 50,000 generations with no burnin and sampling every 500 generations.
From this chain we extracted: 1) the maximum burnin time as 1.1 times the number of generations required to reach the median likelihood value; and 2) the median posteriors of the model (discarding the burnin) as priors for our mini chain models with a belief parameter value of 5%.
We then ran 50 mini chains for 10 samples each using the previous sampling rate (500 generations) and the extracted parameters from the parameterisation chain (the burnin and priors described above).

```{r, echo = FALSE, eval = FALSE}
## Making the parameterisation chains
parameterisation_chains <- make.mini.chains(data = charadriiformes$data,
                                           dimensions = c("PC1", "PC2", "PC3"),
                                           randoms = c("global", "clade"),
                                           residuals = "global",
                                           tree = charadriiformes$tree,
                                           parameters = list(nitt = 50000, burnin = 0, thin = 500))
```

The main arguments here are:

 * `data`: the dataset as described above. If you use the phylogeny or clades as a random term, it needs to have one column called `"animal"` and another one named by the grouping factor (here `"clade"`).
 * `dimensions`: which dimensions from `data` to include. If the argument is a numeric value (like `1:3`) it automatically picks the corresponding columns that have numeric values (here `"PC1"`, `"PC2"` and `"PC3"`). However, you can also directly specify the name of the column (e.g. `c("PC1", "PC2", "PC3")` would give the same results).
 * `tree`: the tree to use. Here we use only one tree (the consensus one) but later we are going to use a tree distribution.
 * `trait.family`: which corresponds to the `family` argument in `MCMCglmm`. Note that if you specify only one family (here `"gaussian"`) it is applied to all the traits (here `"PC1"`, `"PC2"` and `"PC3"`). But you can also specify a vector of families (e.g. `c("gaussian", "poisson", "gaussian")`).
 * `randoms`: which random terms to use. Here `"global"` is the generic term for applying a random term to the whole dataset (the phylogeny) and `"clade` allows to apply another nested random term to the elements described in the column `"clade"` in the dataset. Note that you can play around with many more options for the random terms by looking at the function manual (`?make.mini.chains`).
 * `residuals`: which residual terms to use. This is the same as the `randoms` argument. The `"global"` term is the generic term for applying the residuals to the whole dataset (traits) and no other nested term has been provided.
 * `priors`: setting up the prior. Here you can provide a list of priors in `MCMCglmm` format (see below) or, as we are doing here, simply a flat prior for each term. To provide a flat prior, you just need to give the belief parameter value (here 2%) and the function will generate the correct flat priors attached to this belief.

The other arguments here are either self evident (e.g `verbose`) or the same as for `MCMCglmm` (here `parameters` is just a list of arguments with the correct names to be passed to `MCMCglmm`).
Once the mini chain is ready, you can run it using the `run.mini.chains` function given the mini chain (`param_MCMCglmm`) and a number of replicates (here 3).

```{r, echo = FALSE, eval = FALSE}
## Running the parameterisation chains
param_results <- run.mini.chains(parameterisation_chains, replicates = 3)
## Saving the parameter model results
save(param_results, file = "param_results.rda")
```

> For THE_PAPER we ran three independent parameterisation chains for 100k iterations (`nitt`) with a sampling (`thin`) every 500 iterations.

Once the three chains are computed as a list of parameterisation models, we can visualise the diagnosis status of it.
Note that ideally the parameterisation chains converge, however, there can be a time trade-off between running the model on the consensus tree until it converges and then running the mini-chains: the longer the parameterisation chains the longer the resulting mini chains will take.
Ultimately the focus on the convergence of the mini chains is more important and can be used for further analysis.

```{r, echo = FALSE, eval = TRUE}
## Loading the parameter results
load("param_results.rda")
## Diagnosing the results
diagnose.mini.chains(param_results)
```

Once relatively happy with the diagnosed chains (in this case a potential scale reduction factor higher than 1), we can automatically extract the parameters from the three chains to be used as the parameters for the mini chains:

```{r, echo = FALSE, eval = TRUE}
## Extracting the parameters
params <- extract.parameters(param_results)

## The sampling rate:
sampling <- 500
## The required number of samples per tree
samples <- 10
```

We can then build a mini chain with the same model as previously but using the newly calculated parameters.
In this example, we replicated the chains 100 times across 10 random trees.
We then can use the `combine.mini.chains` function to combine these 100 mini-chains into a single `MCMCglmm` posterior chain. 

> For THE_PAPER we ran the analysis 400 times across 1000 random trees.

```{r, echo = FALSE, eval = FALSE}
## Making the mini chains
parameterisation_chains <- make.mini.chains(data = charadriiformes$data,
                                           dimensions = c("PC1", "PC2", "PC3"),
                                           randoms = c("global", "clade"),
                                           residuals = "global",
                                           priors = params$priors,
                                           tree = charadriiformes$tree_distribution,
                                           parameters = list(
                                            nitt = params$burnin + samples*sampling,
                                            burning = params$burnin,
                                            thin = sampling))
## Running the mini chains
results <- run.mini.chains(parameterisation_chains, replicates = 100)
## Combining all the mini chains into a single posterior chains
posteriors <- combine.mini.chains(results)
## Saving the results
save(posteriors, file = "posteriors.rda")
```

This resulted in obtaining 1000 exploitable posterior variance-covariance matrices for each residual term of our model: the overall phylogenetic effect and the three specific phylogenetic effects for each of the three clades.
These results can then be diagnosed as previously.
However, because the chains were combined, the `diagnose.mini.chains` function doesn't check for a potential scale reduction factor (i.e. a diagnosis of convergence) but rather for the resulting effective sample size in the posteriors.
It is good practice to reach an effective sample size of at least 200, but for the brevity of this example some parameters don't reach that value.

```{r, echo = FALSE, eval = TRUE}
## Loading the combined mini chains
load("posteriors.rda")
## Plotting the posteriors' effective sample sizes
silent <- diagnose.mini.chains(posteriors, plot = TRUE)
```

## Elaboration and innovation analysis

To then analyse these results in the same way we did for THE_PAPER, please refer to [the projections analysis vignette](link) from the `dispRity` package.

## References
